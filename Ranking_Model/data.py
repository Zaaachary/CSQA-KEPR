# -*- encoding: utf-8 -*-
'''
@File    :   data.py
@Time    :   2022/01/06 12:31:47
@Author  :   Zhifeng Li
@Contact :   zaaachary_li@163.com
@Desc    :   
'''

import os
import re
import time
import json
import logging
from typing import Optional
from multiprocessing import Pool, cpu_count     # https://docs.python.org/3/library/multiprocessing.html
from collections import OrderedDict
from itertools import chain
import pickle
# from torch._C import dtype, int32

from tqdm import tqdm
import torch
from torch.utils.data import Dataset, RandomSampler, SequentialSampler, DataLoader, sampler
# https://pytorch.org/tutorials/beginner/basics/data_tutorial.html
import pytorch_lightning as pl

import sys
sys.path.append('../')

# temp rm NLTK
# from Tools.wkdt.match_description import WKDT_matcher
WKDT_matcher = None
from Tools.data_io_util import load_data

logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s')  


class Ranking_Dataset(Dataset):
    '''
    train & dev
        dataset generated by generation Model(GPT-2), will be used to training a ranking model.
    predict
        generation model's predict result, need to be rank.

    '''

    def __init__(self,
        dataset_path, max_seq_len, tokenizer, dataset_type, 
        overwrite_cache=True, ranking_target=None, 
        problem_type='classification_CE', 
        add_wkdt=False, wkdt_path='',
        contrastive_learning=None,
        experiment = '', keyword_path=''
    ):
        '''
        args: dataset_path,
        problem_type: classification_CE, classification_BCE, classification_MCE
        '''
        super().__init__()
        self.example_ids = []
        self.raw_examples = []
        self.labels = []
        self.examples = []

        self.problem_type = problem_type
        self.add_wkdt = add_wkdt
        self.contrastive_learning = contrastive_learning
        self.predict = True if dataset_type == 'predict' else False

        self.tokenizer = tokenizer
        self.dataset_path = dataset_path
        self.keyword_path = keyword_path
        self.dataset_type = dataset_type
        self.ranking_target = ranking_target
        self.max_seq_len = max_seq_len
        self.experiment = experiment
        self.keyword = {}

        if self.add_wkdt:
            self.wkdt = WKDT_matcher(wkdt_path, experiment=self.experiment)

        if self.predict:
            # no label
            # import pdb; pdb.set_trace()
            if 'keyword' in self.experiment:
                self.load_keyword()
            # if 'tsf_predict' in self.experiment:
            # self.load_and_format_data()
            # else:
            self.load_data_for_predict()
            self.convert_tokens_to_ids()
        else:
            # with label
            file_name = f"cached_lm_{dataset_type}.pkl"

            cached_feature_file = os.path.join(
                dataset_path, file_name
            )
            if os.path.exists(cached_feature_file) and not overwrite_cache:
                logging.info(f"loading features from {file_name}")
                with open(cached_feature_file, 'rb') as handle:
                    self.raw_examples = pickle.load(handle)
                    self.examples = pickle.load(handle)
                    self.labels = pickle.load(handle)
            else:
                if 'keyword' in self.experiment:
                    self.load_keyword()
                self.load_and_format_data()
                self.convert_tokens_to_ids()
                # logging.info(f"dumping features to {file_name}")
                # with open(cached_feature_file, 'wb') as handle:
                #     pickle.dump(self.raw_examples, handle)
                #     pickle.dump(self.examples, handle)
                #     pickle.dump(self.labels, handle)
            
            if self.contrastive_learning == "triple":
                self.transfer_to_triple()
            elif 'list_wise' in self.experiment:
                self.transfer_to_listwise()

    def load_keyword(self):
        assert os.path.isdir(self.keyword_path)
        for filename in os.listdir(self.keyword_path):
            file = os.path.join(self.keyword_path, filename)
            content = load_data(file, 'jsonl')
            for example in content:
                example_id = example['metadata']['id']
                keyword = example['keyword']
                self.keyword[example_id] = keyword

    def load_and_format_data(self):
        if not os.path.isfile(self.dataset_path):
            target = os.path.join(self.dataset_path, f"{self.dataset_type}.tsf")
        else:
            target = self.dataset_path
        assert os.path.isfile(target)
        
        with open(target, 'r', encoding='utf-8') as f:
            for index, line in enumerate(f.readlines()):
                line = line.strip().split('\t')
                example_id, question, answer = line[:3]
                if 'weight_scale' in self.experiment or 'soft_label' in self.experiment :
                    label = line[4]
                else:
                    label = line[3]

                if 'commongen' not in self.experiment:
                    question = question.lower()
                    question = self.transform_question(question)

                if self.add_wkdt:
                    answer_wkdt = self.wkdt.match_description(answer)
                    if 'keyword' in self.experiment:
                        keyword_list = self.keyword[example_id]
                        description = []
                        for word in keyword_list:
                            wkdt = self.wkdt.match_description(word)
                            if wkdt['matched'] != "MISMATCH":
                                des = f"{word}: {wkdt['desc_list'][0]}"
                                des = des + '.' if des[-1] != '.' else des
                                description.append(des)
                        if 'top1' in self.experiment:
                            description = description[:1]
                        desc = f' {self.tokenizer.sep_token} '.join(description[:2]) if description else ''
                        
                        if answer_wkdt['matched'] != 'MISMATCH':
                            desc = f"{answer}: {answer_wkdt['desc_list'][0]} {self.tokenizer.sep_token}" + desc
                    else:
                        if answer_wkdt['matched'] != 'MISMATCH':
                            answer = f"{answer} {self.tokenizer.sep_token} {answer}: {answer_wkdt['desc_list'][0]}"

                if 'keyword' in self.experiment:
                    self.raw_examples.append((desc, question + ' '+answer+'.'))
                else:
                    self.raw_examples.append((question, answer))
                self.labels.append(float(label))
                self.example_ids.append(example_id)

        logging.info(f"{self.dataset_type} dataset loaded")

    def load_data_for_predict(self):
        
        if 'commongen' not in self.experiment:
            id2question = {}
            # load origin dataset from dataset_path
            assert os.path.isfile(self.dataset_path)
            f = open(self.dataset_path, 'r', encoding='utf-8')
            for line in f.readlines():
                line = json.loads(line.strip())
                example_id = line['metadata']['id']
                question = line['question']['original'].lower()
                id2question[example_id] = question
            f.close()

            # load answer from ranking_target
            assert os.path.isfile(self.ranking_target)
            f = open(self.ranking_target, 'r', encoding='utf-8')
            for line in f.readlines():
                line = json.loads(line.strip())
                
                for k,v in line.items():
                    if 'r' in k and 'q' in k:
                        example_id, answer_list = k, v
                # TODO normalize answer, remove special token
                question = id2question[example_id]
                question = self.transform_question(question)
                for answer in answer_list:
                    if self.add_wkdt:
                        answer_wkdt = self.wkdt.match_description(answer)
                        if 'keyword' in self.experiment:
                            keyword_list = self.keyword[example_id]
                            description = []
                            for word in keyword_list:
                                wkdt = self.wkdt.match_description(word)
                                if wkdt['matched'] != "MISMATCH":
                                    des = f"{word}: {wkdt['desc_list'][0]}"
                                    des = des + '.' if des[-1] != '.' else des
                                    description.append(des)
                            desc = f' {self.tokenizer.sep_token} '.join(description[:2]) if description else ''
                            if answer_wkdt['matched'] != 'MISMATCH':
                                desc = f"{answer}: {answer_wkdt['desc_list'][0]} {self.tokenizer.sep_token} " + desc
                            else:
                                desc = f"{answer}: {self.tokenizer.sep_token} " + desc
                        else:
                            if answer_wkdt['matched'] != 'MISMATCH':
                                answer = f"{answer} {self.tokenizer.sep_token} {answer} : {answer_wkdt['desc_list'][0]}"
                            
                    if 'keyword' in self.experiment:
                        self.raw_examples.append((desc, question + ' '+answer+'.'))
                    else:
                        self.raw_examples.append((question, answer))
                    self.example_ids.append(example_id)
            f.close()
        else:
            # import pdb; pdb.set_trace()
            assert os.path.isfile(self.ranking_target)
            f = open(self.ranking_target, 'r', encoding='utf-8')
            example_id = 0
            for line in f.readlines():
                line = json.loads(line)
                concept_set = line['concept_set']
                pred_scene = line['pred_scene']
                for scene in pred_scene:
                    self.raw_examples.append((concept_set, scene))
                    self.example_ids.append(example_id)                
                example_id += 1
            f.close()
        logging.info(f"{self.dataset_path} dataset loaded")

    class Convert:
        # tokenize and get the max len   for multiprocessing
        def __init__(self, tokenizer, max_seq_len):
            self.tokenizer = tokenizer
            self.max_seq_len = max_seq_len

        def __call__(self, raw_example):
            feature_dict = self.tokenizer.encode_plus(*raw_example, max_length=self.max_seq_len, truncation='longest_first', padding='max_length')
            # feature_dict = self.tokenizer.encode_plus(*raw_example, max_length=self.max_seq_len, truncation='only_first', padding='max_length')longest_first
            return feature_dict

    def convert_tokens_to_ids(self):
        # single thread processing
        # for example in tqdm(self.raw_examples):
        #     feature_dict = self.tokenizer.encode_plus(*example, max_length=self.max_seq_len, truncation='only_first', padding='max_length')
        #     self.examples.append(feature_dict)
        # Multithread processing
        logging.info(f"tokenizing {self.dataset_type} examples")
        logging.info(f"example: {self.raw_examples[0]}")
        now = time.time()
        with Pool(processes=min(4, cpu_count())) as pool:
            tokenized_examples = pool.map(
                self.Convert(self.tokenizer, self.max_seq_len),
                self.raw_examples)
            self.examples = tokenized_examples
        logging.info(f"run {min(4, cpu_count())} thread; process {len(self.examples)} examples; cost {time.time() - now}")

    def transfer_to_listwise(self):
        # group cases with same example_ids
        # self.examples_ids, self.examples, self.labels -> 
        max_answer_num = 10
        continue_num = 0
        example_dict = {}
        for index, (example_id, label) in enumerate(zip(self.example_ids, self.labels)):
            example = example_dict.get(example_id, {'index_list':[], 'labels':[]})
            example['index_list'].append(index)
            example['labels'].append(label)
            example_dict[example_id] = example

        self.example_ids.clear()
        examples = []
        labels = []
        for example_id, value in example_dict.items():
            if len(value['labels']) > max_answer_num:
                # TODO more than max_answer_num
                continue_num += 1
                continue
            self.example_ids.append(example_id)
            example = {'input_ids':[], 'token_type_ids':[], 'attention_mask':[]}
            for index in value['index_list']:
                feature_dict = self.examples[index]
                for key, v in feature_dict.items():
                    example[key].append(v)
            labels.append(value['labels'])
            examples.append(example)

        self.examples = examples
        self.labels = labels
        logging.info(f"skip {continue_num} question")

        # replenish
        for index, (example, label) in enumerate(zip(self.examples, self.labels)):
            gap = max_answer_num - len(example['input_ids'])
            if gap > 0 :
                for k, v in example.items():
                    example[k].extend([[0] * self.max_seq_len for _ in range(gap)])
                label.extend([-1]*gap)

            self.examples[index] = example
            self.labels[index] = label

    def transfer_to_triple(self):

        def replenish(current):
            if len(current['right'])%2:
                current['right'].append(current['right'][0])
            index = 0
            while len(current['right']) > len(current['wrong']):
                current['wrong'].append(current['wrong'][index])
                index += 1
            while len(current['right']) < len(current['wrong']):
                current['wrong'].pop()

        # sort examples to cluster
        question_cluster = []
        current_id = self.example_ids[0]
        current_question = {'right': [], 'wrong': []}
        for index, (example_id, label) in enumerate(zip(self.example_ids, self.labels)):
            if example_id == current_id:
                if label == 1:
                    current_question['right'].append(index)
                else:
                    current_question['wrong'].append(index)
            else:
                replenish(current_question)
                question_cluster.append(current_question)
                current_id = example_id
                current_question = {'right': [], 'wrong': []}
        else:
            replenish(current_question)
            question_cluster.append(current_question)
        # stack 4 cases into a example  qa, qa+, qa-, cls_only qa
        total_examples = []
        labels = []
        for question in question_cluster:
            for i in range(0, len(question['right'])//2, 2):
                example = []
                label = [1, 1, 0, 0]
                example.append(question['right'][i])
                example.append(question['right'][i+1])
                example.append(question['wrong'][i])
                example.append(question['wrong'][i+1])
                total_examples.append(example)
                labels.append(label)
        # map index to ids
        for index, example in enumerate(total_examples):
            temp = {'input_ids':[], 'token_type_ids':[], 'attention_mask':[]}
            for idx in example:
                feature_dict = self.examples[idx]
                for key, value in feature_dict.items():
                    temp[key].append(value)
            total_examples[index] = temp
        self.examples = total_examples
        self.labels = labels

    def make_dataloader(self, batch_size):
        if self.dataset_type == "train" and 'no_shuffle' not in self.experiment:
            data_sampler = RandomSampler(self)
        else:
            data_sampler = SequentialSampler(self)

        dataloader = DataLoader(
            self, sampler=data_sampler, 
            batch_size=batch_size, num_workers=4,
            collate_fn=self.collate_fn
            )

        return dataloader

    @staticmethod
    def transform_question(origin):
        '''
        > after having kids name something that happens that interrupts a couples alone time at night

        > after having kids one thing that happens that interrupts a couples alone time at night is

        '''
        question = origin.lower()
        question = question.replace('.', '')
        question = question.replace(':', '')
        question = question.replace('?', '')
        question = question.replace('someone', 'one person')
        question = question.replace('someplace', 'one place')
            # "name": "",
        transform_dict = {
            "name something": "one thing",
            'tell me something': 'one thing',
            'name a ': 'one ',
            "name an ": "one ",
            "name": '',
            "SW tell me a ": "one ",
            "SW tell me an ": "one ",
            "SW what": "one",
            "SW give me a ": "one ",
            "SW tell me ": "",
            "which": "one",
            "what": "one",
            "how can you tell": "one way to tell",
        }
        order = ['name something', 'tell me something', 'name a ', 'name an ', 'name',
            'SW tell me a ', 'SW tell me an ', 'SW what', 'SW give me a ', 'SW tell me ',
            'which', 'what', 'how can you tell']
        transform = OrderedDict.fromkeys(order)
        transform.update(transform_dict)
        
        for pattern, trans in transform.items():
            if pattern.startswith('SW') and pattern[3:] in question:
                question = question.replace(pattern[3:], trans)
                question = question.strip() + ' is'
                break
            elif pattern in question:
                question = question.replace(pattern, trans)
                question = question.strip() + ' is'
                break

        else:
            question = 'Q: ' + question +'? A: '

        question = question[0].upper() + question[1:]

        return question

    def collate_fn(self, batch):
        pad_token_id = self.tokenizer.pad_token_id
        max_inputids_len = 0
        for example in batch:
            input_ids = example['input_ids']
            for index, token in enumerate(input_ids):
                if token == pad_token_id:
                    max_inputids_len = max(max_inputids_len, index)
                    break
            
        input_ids = []
        masks = []
        token_type_ids = []
        labels = []
        example_ids = []

        for example in batch:
            input_ids.append(example['input_ids'][:max_inputids_len])
            masks.append(example['attention_mask'][:max_inputids_len])
            token_type_ids.append(example['token_type_ids'][:max_inputids_len])
            if not self.predict:
                labels.append(example['labels'])
            if self.dataset_type == 'dev':
                example_ids.append(example['example_id'])
        
        input_ids = torch.stack(input_ids)
        masks = torch.stack(masks)
        token_type_ids = torch.stack(token_type_ids)
        if not self.predict:
            labels = torch.stack(labels)
                
        batch = {
            "input_ids": input_ids,
            "attention_mask": masks,
            "token_type_ids": token_type_ids,
            "labels": labels,
            'example_id': example_ids
        }
        if self.dataset_type != 'dev':
            del batch['example_id']
        if self.predict:
            del batch['labels']
            
        # import pdb; pdb.set_trace()
        return batch

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, idx):
        feature_dict = self.examples[idx]
        for key, value in feature_dict.items():
            feature_dict[key] = torch.tensor(value)

        if not self.predict:
            if self.problem_type in ["regression", "classification_BCE"]:
                feature_dict['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)
            elif self.problem_type in ["classification_CE", "classification_MCE"]:
                feature_dict['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)

        if self.dataset_type == 'dev':
            feature_dict['example_id'] = self.example_ids[idx]
        return feature_dict


class CSQA_Dataset(Dataset):

    def __init__(self, 
        dataset_path, max_seq_len, tokenizer, dataset_type, problem_type):
        super().__init__()
        self.example_ids = []
        self.raw_examples = []
        self.labels = []
        self.examples = []

        self.dataset_type = dataset_type
        self.dataset_path = dataset_path
        self.max_seq_len = max_seq_len
        self.tokenizer = tokenizer
        self.problem_type = problem_type

        self.load_data()
        self.convert_tokens_to_ids()
        
    def load_data(self):
        if not os.path.isfile(self.dataset_path):
            target = os.path.join(self.dataset_path, f"{self.dataset_type}_rand_split.jsonl")
        assert os.path.isfile(target)

        with open(target, 'r', encoding='utf-8') as f:
            for line in f.readlines():
                example = json.loads(line)
                example['answerKey']
                self.labels.append(ord(example['answerKey']) - ord('A'))
                self.example_ids.append(example['id'])

                question = example['question']['stem'].strip().lower()
                question = question[0].upper() + question[1:]
                answers = []
                for ans in example['question']['choices']:
                    answers.append(ans['text'])
                self.raw_examples.append((question, answers))

        logging.info(f"{self.dataset_type} dataset loaded")

    class Convert:
        # tokenize and get the max len   for multiprocessing
        def __init__(self, tokenizer, max_seq_len):
            self.tokenizer = tokenizer
            self.max_seq_len = max_seq_len

        def __call__(self, raw_example):
            question, temp = raw_example[0], []
            for ans in raw_example[1]:
                temp.append((question, ans))
            feature_dict = self.tokenizer.batch_encode_plus(temp, max_length=self.max_seq_len, truncation='only_first', padding='max_length')
            return feature_dict

    def convert_tokens_to_ids(self):
        logging.info(f"tokenizing {self.dataset_type} examples")
        now = time.time()
        # single thread processing  take 23s
        # for example in tqdm(self.raw_examples):
        #     question, temp = example[0], []
        #     for ans in example[1]:
        #         temp.append((question, ans))
        #     feature_dict = self.tokenizer.batch_encode_plus(temp, max_length=self.max_seq_len, truncation='only_first', padding='max_length')
        #     self.examples.append(feature_dict)
        # Multithread processing
        with Pool(processes=min(4, cpu_count())) as pool:
            tokenized_examples = pool.map(
                self.Convert(self.tokenizer, self.max_seq_len),
                self.raw_examples)
            self.examples = tokenized_examples
        # import pdb; pdb.set_trace()
        logging.info(f"run {min(4, cpu_count())} thread; process {len(self.examples)} examples; cost {time.time() - now}")

    def make_dataloader(self, batch_size):
        if self.dataset_type == "train":
            # data_sampler = RandomSampler(self)
            data_sampler = SequentialSampler(self)
        else:
            data_sampler = SequentialSampler(self)
        dataloader = DataLoader(self, sampler=data_sampler, batch_size=batch_size, num_workers=4)
        return dataloader

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, idx):
        feature_dict = self.examples[idx]
        for key, value in feature_dict.items():
            feature_dict[key] = torch.tensor(value)
        feature_dict['labels'] = torch.tensor(self.labels[idx])
        return feature_dict


if __name__ == "__main__":
    '''
    for testing
    '''
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained("/SISDC_GPFS/Home_SE/hy-suda/zfli/Models/init_model/albert-xxlarge-v2")
    dataset = Ranking_Dataset(
        dataset_path="/SISDC_GPFS/Home_SE/hy-suda/zfli/CODE/ProtoQA/proto-qa-research/DATA/ranking_data/BC_Hard_Case/v2.1fix" , 
        max_seq_len=120, 
        tokenizer=tokenizer, 
        dataset_type='dev', 
        overwrite_cache=True, ranking_target=None, 
        problem_type='classification_BCE', 
        add_wkdt=True, wkdt_path='/SISDC_GPFS/Home_SE/hy-suda/zfli/CODE/ProtoQA/proto-qa-research/DATA/wiktionary.dict',
        contrastive_learning=None,
        experiment = 'keyword_pow', keyword_path='/SISDC_GPFS/Home_SE/hy-suda/zfli/CODE/ProtoQA/proto-qa-research/DATA/generation_data/with_keyword_v1'
        )
    batch = [dataset[0], dataset[1]]
    dataset.collate_fn(batch)
    
    
